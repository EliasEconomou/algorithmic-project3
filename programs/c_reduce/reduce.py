# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x3MQZtJ3CXMx0Bl8ooCoOSZw0jyBzYoz
"""

# --------\
# IMPORTS  |
# --------/
import sys
import math
import matplotlib.pyplot as plt
import keras
import pandas as pd
import numpy as np
import datetime
import time
import requests as req
import json
import pickle
import os
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.layers import *
from keras.callbacks import EarlyStopping
from keras.layers import Input, Dense, Conv1D, MaxPooling1D, UpSampling1D, BatchNormalization, LSTM, RepeatVector
from keras.models import Model
from keras.models import model_from_json
from keras import regularizers
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

# --------------------------------\
# STARTING VARIABLES AND ARGUMENTS |
# --------------------------------/
window = 50
encoding_dim = 3
epochs = 10
test_samples = 200
dataset_path = "nasdaq2007_17.csv"
output_dataset_file ="nasdaq2007_17_ENCODED.csv"

for i in range(len(sys.argv)):
  if(sys.argv[i] == "-od"):
    output_dataset_file = sys.argv[i+1]
  elif(sys.argv[i] == "-oq"):
    output_query_file = sys.argv[i+1]
  elif(sys.argv[i] == "-q"):
    queryset = sys.argv[i+1]
  elif(sys.argv[i] == "-d"):
    dataset_path = sys.argv[i+1]

# ---------------------\
# PLOT MAKING FUNCTIONS |
# ---------------------/

def plot_examples(stock_input, stock_decoded):
    n = 10  
    plt.figure(figsize=(20, 4))
    for i, idx in enumerate(list(np.arange(0, test_samples, 20))):
        # display original
        ax = plt.subplot(2, n, i + 1)
        if i == 0:
            ax.set_ylabel("Input", fontweight=600)
        else:
            ax.get_yaxis().set_visible(False)
        plt.plot(stock_input[idx])
        ax.get_xaxis().set_visible(False)
        

        # display reconstruction
        ax = plt.subplot(2, n, i + 1 + n)
        if i == 0:
            ax.set_ylabel("Output", fontweight=600)
        else:
            ax.get_yaxis().set_visible(False)
        plt.plot(stock_decoded[idx])
        ax.get_xaxis().set_visible(False)

def plot_all(stock_input, stock_decoded):
  for i in range(stock_input.shape[0]):
    # Visualising the results
    plt.plot(stock_input[i], color = "red", label = "Real Value")
    plt.plot(stock_decoded[i], color = "blue", label = "Decoded Value")
    # plt.xticks(np.arange(0,len(dataset_test),100))
    # plt.title('Stock Price Prediction for "'+df.columns[series_number]+'" time series')
    plt.xlabel('Time')
    plt.ylabel('Value')
    plt.legend()
    plt.show()
        
     
        
def plot_history(history):
    plt.figure(figsize=(15, 5))
    ax = plt.subplot(1, 2, 1)
    plt.plot(history.history["loss"])
    plt.title("Train loss")
    ax = plt.subplot(1, 2, 2)
    plt.plot(history.history["val_loss"])
    plt.title("Test loss")

# --------------------\
# GETTING DATA         |
# --------------------/
df=pd.read_csv(dataset_path, header=None, delimiter='\t') #create data frame from our csv file

df = df.transpose() # transpose rows to columns

# Renaming header as the time series' ids
df.columns = df.iloc[0]
df = df.reindex(df.index.drop(0)).reset_index(drop=True)
df.columns.name = None

# ----------------------------------------\
# TRAINING SET, TESTING SET AND TOTAL SET  |
# ----------------------------------------/
train_series_X = []
total_series = []

num_series_selected = df.shape[1]
print("Number of selected series to train: ",num_series_selected)
series_to_train = []
for i in range(num_series_selected):
  series_to_train.append(i)
# print(series_to_train)

for series_number in series_to_train:
  split = int(df.shape[0])
  training_set = df.iloc[:split, series_number:series_number+1].values  #training set will be the first 80% rows
  
  # Feature Scaling
  sc = MinMaxScaler(feature_range = (0, 1))
  training_set_scaled = sc.fit_transform(training_set)
  

  X_train = []
  for i in range(window, split):
      X_train.append(training_set_scaled[i-window:i, 0])

  for i in range(0, split, window):
      total_series.append(training_set_scaled[i:i+window,0])

  
  train_series_X = train_series_X + X_train

# Reshape input
total_series = np.array(total_series)
total_series = np.reshape(total_series, (total_series.shape[0], total_series.shape[1], 1))

train_series_X = np.array(train_series_X)
train_series_X = np.reshape(train_series_X, (train_series_X.shape[0], train_series_X.shape[1], 1))

split = int(0.8*df.shape[0])
X_test = total_series[-split:]

train_series_X = train_series_X[:-split]






# -----------------------------------------------------------------------------------------------------------------#
#   !!!    BETWEEN THE DOUBLE LINES IS THE TRAINING PROCCESS, NOT NEEDED SINCE ALREADY CREATED AUTOENCODER     !!! #
# -----------------------------------------------------------------------------------------------------------------#

# # --------------------\
# # TRAINING AUTOENCODER |
# # --------------------/

# input_window = Input(shape=(window,1))

# x = Conv1D(16, 3, activation="relu", padding="same")(input_window) # 10 dims
# # x = BatchNormalization()(x)
# x = MaxPooling1D(2, padding="same")(x) # 5 dims
# x = Conv1D(1, 3, activation="relu", padding="same")(x) # 5 dims
# # x = BatchNormalization()(x)
# encoded = MaxPooling1D(2, padding="same")(x) # 3 dims

# encoder = Model(input_window, encoded)

# # 3 dimensions in the encoded layer

# x = Conv1D(1, 3, activation="relu", padding="same")(encoded) # 3 dims
# # x = BatchNormalization()(x)
# x = UpSampling1D(2)(x) # 6 dims
# x = Conv1D(16, 2, activation='relu')(x) # 5 dims
# # x = BatchNormalization()(x)
# x = UpSampling1D(2)(x) # 10 dims
# decoded = Conv1D(1, 3, activation='sigmoid', padding='same')(x) # 10 dims
# autoencoder = Model(input_window, decoded)
# autoencoder.summary()

# autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

# history = autoencoder.fit(train_series_X, train_series_X,
#                 epochs=epochs,
#                 batch_size=32,
#                 shuffle=True,
#                 validation_data=(X_test, X_test))

# # --------------------\
# # SAVING AUTOENCODER   |
# # --------------------/

# autoencoder.save('autoencoder_10_epochs_32_batchsize_3_dims_50_window_100_Data.h5')
# print("SUCCESSFULLY SAVED AUTOENCODER")

# -----------------------------------------------------------------------------------------------------------------#
#   !!!    BETWEEN THE DOUBLE LINES IS THE TRAINING PROCCESS, NOT NEEDED SINCE ALREADY CREATED AUTOENCODER     !!! #
# -----------------------------------------------------------------------------------------------------------------#






# --------------------\
# LOADING AUTOENCODER  |
# --------------------/
autoencoder = keras.models.load_model('autoencoder_10_epochs_32_batchsize_3_dims_50_window_100_Data.h5')

# ------------------------------------------\
# USING AUTOENCODER X_TEST (LAST 20% OF DATA)|
# ------------------------------------------/
decoded_data = autoencoder.predict(total_series)
print("SUCCESSFULLY DECODED DATA")

# -----------------------------------\
# HISTORY AND EXAMPLES AS IN TUTORIAL |
# -----------------------------------/

# plot_history(history)

plot_examples(total_series, decoded_data)

