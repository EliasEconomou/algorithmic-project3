# -*- coding: utf-8 -*-
"""forecast_save.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UpsnvCvpZg1r3DxAtRcrF14FRYoZ2F2D
"""



import sys
import math
import matplotlib.pyplot as plt
import keras
import pandas as pd
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.layers import *
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from keras.callbacks import EarlyStopping

window = 40                 # default number of time steps when training
num_epochs = 15                # default number of epochs used when training the model                     
predict_series = 10         # default number of series to predict
dataset_path = "nasdaq2007_17.csv"
# To train time series 'per series' method is 1
# To train time series 'as a set' method is 2
method = 2 # default method is 2

for i in range(len(sys.argv)):
  if(sys.argv[i] == "-n"):
    predict_series = int(sys.argv[i+1])
  elif(sys.argv[i] == "-w"):
    window = int(sys.argv[i+1])
  elif(sys.argv[i] == "-e"):
    epochs = int(sys.argv[i+1])
  elif(sys.argv[i] == "-m"):
    method = int(sys.argv[i+1])
  elif(sys.argv[i] == "-d"):
    dataset_path = sys.argv[i+1]

df=pd.read_csv(dataset_path, header=None, delimiter='\t') #create data frame from our csv file

df = df.transpose() # transpose rows to columns

# Renaming header as the time series' ids
df.columns = df.iloc[0]
df = df.reindex(df.index.drop(0)).reset_index(drop=True)
df.columns.name = None

"""Method 1: Train per series"""

if method == 1: #in training per series we will train and predict n given series
  num_series_selected = predict_series 
  
  series_to_train = []
  for i in range(num_series_selected):
    series_to_train.append(i)
  print(series_to_train)
  
  for series_number in series_to_train:

    split = int(0.8*df.shape[0])
    training_set = df.iloc[:split, series_number:series_number+1].values  #training set will be the first 80% rows

    # Feature Scaling
    sc = MinMaxScaler(feature_range = (0, 1))
    training_set_scaled = sc.fit_transform(training_set)

    X_train = []
    Y_train = []
    for i in range(window, split):
        X_train.append(training_set_scaled[i-window:i, 0])
        Y_train.append(training_set_scaled[i, 0])

    # Reshape input
    X_train, Y_train = np.array(X_train), np.array(Y_train)
    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

    # Training the model
    model = Sequential()
    #Adding the first LSTM layer and some Dropout regularisation
    model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))
    model.add(Dropout(0.2))
    # Adding a second LSTM layer and some Dropout regularisation
    model.add(LSTM(units = 50, return_sequences = True))
    model.add(Dropout(0.2))
    # Adding a third LSTM layer and some Dropout regularisation
    model.add(LSTM(units = 50, return_sequences = True))
    model.add(Dropout(0.2))
    # Adding a fourth LSTM layer and some Dropout regularisation
    model.add(LSTM(units = 50))
    model.add(Dropout(0.2))
    # Adding the output layer
    model.add(Dense(units = 1))

    # Compiling the RNN
    model.compile(optimizer = 'adam', loss = 'mean_squared_error')

    # Fitting the RNN to the Training set
    model.fit(X_train, Y_train, epochs = num_epochs, batch_size = 64)


    # Preparing the testing data 
    dataset_train = df.iloc[:split, series_number:series_number+1]
    dataset_test = df.iloc[split:, series_number:series_number+1]

    dataset_total = pd.concat((dataset_train, dataset_test), axis = 0)
    inputs = dataset_total[len(dataset_total) - len(dataset_test) - window:].values
    inputs = inputs.reshape(-1,1)
    inputs = sc.transform(inputs)
    X_test = []
    for i in range(window, len(dataset_test)+window):
        X_test.append(inputs[i-window:i, 0])
    X_test = np.array(X_test)
    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

    predicted_stock_price = model.predict(X_test)
    predicted_stock_price = sc.inverse_transform(predicted_stock_price)

    # Visualising the results
    plt.plot(dataset_test.values, color = "red", label = "Real Value")
    plt.plot(predicted_stock_price, color = "blue", label = "Predicted Value")
    plt.xticks(np.arange(0,len(dataset_test),100))
    plt.title('Stock Price Prediction for "'+df.columns[series_number]+'" time series')
    plt.xlabel('Time')
    plt.ylabel('Stock Price')
    plt.legend()
    plt.show()

"""Method 2 : Train as a set"""

if method == 2: #in training as a set we will train with all series and predict n given series
  # Concat all time series into one big series to train
  train_series_X = []
  train_series_Y = []
  
  num_series_selected = 100
  print("Number of selected series to train: ",num_series_selected)
  series_to_train = []
  for i in range(num_series_selected):
    series_to_train.append(i)

  for series_number in series_to_train:
    split = int(0.8*df.shape[0])
    training_set = df.iloc[:split, series_number:series_number+1].values  #training set will be the first 80% rows
    
    # Feature Scaling
    sc = MinMaxScaler(feature_range = (0, 1))
    training_set_scaled = sc.fit_transform(training_set)

    X_train = []
    Y_train = []
    for i in range(window, split):
        X_train.append(training_set_scaled[i-window:i, 0])
        Y_train.append(training_set_scaled[i, 0])
    

    
    train_series_X = train_series_X + X_train
    train_series_Y = train_series_Y + Y_train

  # Reshape input
  train_series_X, train_series_Y = np.array(train_series_X), np.array(train_series_Y)
  train_series_X = np.reshape(train_series_X, (train_series_X.shape[0], train_series_X.shape[1], 1))

  # Training the model
  model = Sequential()
  #Adding the first LSTM layer and some Dropout regularisation
  model.add(LSTM(units = 50, return_sequences = True, input_shape = (train_series_X.shape[1], 1)))
  model.add(Dropout(0.2))
  # Adding a second LSTM layer and some Dropout regularisation
  model.add(LSTM(units = 50, return_sequences = True))
  model.add(Dropout(0.2))
  # Adding a third LSTM layer and some Dropout regularisation
  model.add(LSTM(units = 50, return_sequences = True))
  model.add(Dropout(0.2))
  # Adding a fourth LSTM layer and some Dropout regularisation
  model.add(LSTM(units = 50))
  model.add(Dropout(0.2))
  # Adding the output layer
  model.add(Dense(units = 1))

  # Compiling the RNN
  model.compile(optimizer = 'adam', loss = 'mean_squared_error')

  # Fitting the RNN to the Training set
  model.fit(train_series_X, train_series_Y, epochs = num_epochs, batch_size = 32)

  model.save('forecast_model_v1.h5')

  # model = keras.models.load_model('forecast_model_v3.h5')

  for series_number in range(predict_series):

    # Preparing the testing data 
    dataset_train = df.iloc[:split, series_number:series_number+1]
    dataset_test = df.iloc[split:, series_number:series_number+1]

    dataset_total = pd.concat((dataset_train, dataset_test), axis = 0)
    inputs = dataset_total[len(dataset_total) - len(dataset_test) - window:].values
    inputs = inputs.reshape(-1,1)
    inputs = sc.transform(inputs)
    X_test = []
    for i in range(window, len(dataset_test)+window):
        X_test.append(inputs[i-window:i, 0])
    X_test = np.array(X_test)
    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

    predicted_stock_price = model.predict(X_test)
    predicted_stock_price = sc.inverse_transform(predicted_stock_price)

    # Visualising the results
    plt.plot(dataset_test.values, color = "red", label = "Real Value")
    plt.plot(predicted_stock_price, color = "blue", label = "Predicted Value")
    plt.xticks(np.arange(0,len(dataset_test),100))
    plt.title('Stock Price Prediction for "'+df.columns[series_number]+'" time series')
    plt.xlabel('Time')
    plt.ylabel('Stock Price')
    plt.legend()
    plt.show()

